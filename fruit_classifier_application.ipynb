{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Fruit Identification App \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step3): Create a CNN to Classify Fruit types (from Scratch)\n",
    "* [Step 2](#step4): Create a CNN to Classify Fruit types (using Transfer Learning)\n",
    "* [Step 3](#step5): Write your Algorithm\n",
    "* [Step 4](#step6): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 76824 total fruit images.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import os\n",
    "#import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline    \n",
    "import os\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  \n",
    "\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load filenames for human and dog images\n",
    "fruit_files = np.array(glob(\"images/*/*/*\"))\n",
    "\n",
    "# print number of images in each dataset\n",
    "print('There are %d total fruit images.' % len(fruit_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Fruits\n",
    "\n",
    "In this section, we use a [pre-trained model](http://pytorch.org/docs/master/torchvision/models.html) to detect fruits in images.  \n",
    "\n",
    "### Obtain Pre-trained VGG-16 Model\n",
    "\n",
    "The code cell below downloads the VGG-16 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.torch/models/vgg16-397923af.pth\n",
      "100%|██████████| 553433881/553433881 [00:22<00:00, 24652780.22it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define VGG16 model\n",
    "VGG16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# move model to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    VGG16 = VGG16.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an image, this pre-trained VGG-16 model returns a prediction (derived from the 1000 possible categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Making Predictions with a Pre-trained Model\n",
    "\n",
    "In the next code cell, you will write a function that accepts a path to an image (such as `'image/Training/*/*.jpg'`) as input and returns the index corresponding to the ImageNet class that is predicted by the pre-trained VGG-16 model.  The output should always be an integer between 0 and 999, inclusive.\n",
    "\n",
    "Before writing the function, make sure that you take the time to learn  how to appropriately pre-process tensors for pre-trained models in the [PyTorch documentation](http://pytorch.org/docs/stable/torchvision/models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def VGG16_predict(img_path):\n",
    "    '''\n",
    "    Use pre-trained VGG-16 model to obtain index corresponding to \n",
    "    predicted ImageNet class for image at specified path\n",
    "    \n",
    "    Args:\n",
    "        img_path: path to an image\n",
    "        \n",
    "    Returns:\n",
    "        Index corresponding to VGG-16 model's prediction\n",
    "    '''\n",
    "    \n",
    "    ## Load and pre-process an image from the given img_path\n",
    "    ## Return the *index* of the predicted class for that image\n",
    "    image = Image.open(img_path)\n",
    "    \n",
    "    # Define transformations for the image\n",
    "    transformation = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Preprocess the image\n",
    "    image_t = transformation(image).float()\n",
    "\n",
    "    # Add extra dimension to treat as batch\n",
    "    image_t = image_t.unsqueeze_(0)\n",
    "    \n",
    "    # Check appropriate device type\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #Send to GPU if available\n",
    "    image_t = image_t.to(device)  \n",
    "    # Turn the input into a Variable\n",
    "    image_model_input = Variable(image_t)\n",
    "    \n",
    "    #Prepare model\n",
    "    VGG16.to(device).eval()\n",
    "    output = VGG16(image_model_input)\n",
    "    \n",
    "    #Get values and index of max value\n",
    "    values, index = torch.max(output, 1)\n",
    "    return index.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Write a Fruit Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), we will return the index of the ImageNet class from the function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a fruit is detected in the image stored at img_path\n",
    "def fruit_detector(img_path):\n",
    "    ## TODO: Complete the function.\n",
    "    index = VGG16_predict(img_path)\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Test Observations) Results from pretrained model\n",
    "\n",
    "Some fruit classes look to perform very well, but others are very poor, like Bananas, Lemons, and Oranges.  We will create a custom model training to try to improve our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of apples images detected as an apple: 100\n",
      "Percent of bananas images detected as a banana: 9\n",
      "Percent of strawbery images detected as a strawbery: 100\n",
      "Percent of orange images detected as a orange: 33\n",
      "Percent of Lemon images detected as a Lemon: 10\n"
     ]
    }
   ],
   "source": [
    "# Test fruit detector function\n",
    "# Not all the fruit classes are in the ImageNet dataset, pick a few known ones and evaluate\n",
    "\n",
    "fruit_detector_fruit_count = 0\n",
    "\n",
    "appleFiles = np.array(glob(\"/home/workspace/images/*/Apple Granny Smith/*\"))\n",
    "apple_files_short = fruit_files_short = appleFiles[:100]\n",
    "\n",
    "for path in apple_files_short:\n",
    "    # add bounding box to color image \n",
    "    resultindex = fruit_detector(path)\n",
    "    if resultindex == 948:\n",
    "        fruit_detector_fruit_count+=1\n",
    "\n",
    "print(\"Percent of apples images detected as an apple: %d\" % (fruit_detector_fruit_count))\n",
    "\n",
    "fruit_detector_fruit_count = 0 \n",
    "\n",
    "banana_files = np.array(glob(\"/home/workspace/images/*/Banana/*\"))\n",
    "banana_files_short = banana_files[:100]\n",
    "\n",
    "for path in banana_files_short:\n",
    "    resultindex = fruit_detector(path)\n",
    "    if resultindex == 954:\n",
    "        fruit_detector_fruit_count+=1\n",
    "\n",
    "print(\"Percent of bananas images detected as a banana: %d\" % (fruit_detector_fruit_count))\n",
    "\n",
    "fruit_detector_fruit_count = 0 \n",
    "\n",
    "strawbery_files = np.array(glob(\"/home/workspace/images/*/Strawberry/*\"))\n",
    "strawbery_files_short = strawbery_files[:100]\n",
    "\n",
    "for path in strawbery_files_short:\n",
    "    resultindex = fruit_detector(path)\n",
    "    if resultindex == 949:\n",
    "        fruit_detector_fruit_count+=1\n",
    "\n",
    "print(\"Percent of strawbery images detected as a strawbery: %d\" % (fruit_detector_fruit_count))\n",
    "\n",
    "fruit_detector_fruit_count = 0 \n",
    "\n",
    "orange_files = np.array(glob(\"/home/workspace/images/*/Orange/*\"))\n",
    "orange_files_short = orange_files[:100]\n",
    "\n",
    "for path in orange_files_short:\n",
    "    resultindex = fruit_detector(path)\n",
    "    if resultindex == 950:\n",
    "        fruit_detector_fruit_count+=1\n",
    "\n",
    "print(\"Percent of orange images detected as a orange: %d\" % (fruit_detector_fruit_count))\n",
    "\n",
    "\n",
    "fruit_detector_fruit_count = 0 \n",
    "\n",
    "lemon_files = np.array(glob(\"/home/workspace/images/*/Lemon/*\"))\n",
    "lemon_files_short = lemon_files[:100]\n",
    "\n",
    "for path in lemon_files_short:\n",
    "    resultindex = fruit_detector(path)\n",
    "    if resultindex == 951:\n",
    "        fruit_detector_fruit_count+=1\n",
    "\n",
    "print(\"Percent of Lemon images detected as a Lemon: %d\" % (fruit_detector_fruit_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create a CNN to Classify Fruits (from Scratch)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Specify Data Loaders for the Fruits Dataset\n",
    "\n",
    "To create data augmentation, we will apply transforms to the training set that will rotate the input images, flip them and apply some color adjustsments.  We will also normalize the input images and scale them all to 100x100 pixels.  This will improve our results as it will simulate more input for training and standarize scaling of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_transform = transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "\n",
    "resize_transform = transforms.Resize(100)\n",
    "centercrop_transform = transforms.CenterCrop(100)\n",
    "\n",
    "data_transforms = {\n",
    "    'Training': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(100),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        normalize_transform\n",
    "    ]),\n",
    "    'Validation': transforms.Compose([\n",
    "        resize_transform,\n",
    "        centercrop_transform,\n",
    "        transforms.ToTensor(),\n",
    "        normalize_transform\n",
    "    ]), \n",
    "    'Test': transforms.Compose([\n",
    "        resize_transform,\n",
    "        centercrop_transform,\n",
    "        transforms.ToTensor(),\n",
    "        normalize_transform\n",
    "    ]),    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Loading the images from the \"Test\" and \"Train\" directories.  We will use 20% of the training data as a validation set\n",
    "\n",
    "Create a CNN to classify fruit class.  Use the template in the code cell below.  Assumption is that images from Kaggle have been placed in the \"images\" folder and there is a \"Training\" and \"Test\" directory.  We will adjust our training set to use 20% for data validation.  Since our classes our imbalanced (some classes have more images than others), we will apply a WeightedSampler to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 57276\n",
      "Testing 19548\n",
      "['Apple Braeburn', 'Apple Crimson Snow', 'Apple Golden 1', 'Apple Golden 2', 'Apple Golden 3', 'Apple Granny Smith', 'Apple Pink Lady', 'Apple Red 1', 'Apple Red 2', 'Apple Red 3', 'Apple Red Delicious', 'Apple Red Yellow 1', 'Apple Red Yellow 2', 'Apricot', 'Avocado', 'Avocado ripe', 'Banana', 'Banana Lady Finger', 'Banana Red', 'Blueberry', 'Cactus fruit', 'Cantaloupe 1', 'Cantaloupe 2', 'Carambula', 'Cherry 1', 'Cherry 2', 'Cherry Rainier', 'Cherry Wax Black', 'Cherry Wax Red', 'Cherry Wax Yellow', 'Chestnut', 'Clementine', 'Cocos', 'Dates', 'Ginger Root', 'Granadilla', 'Grape Blue', 'Grape Pink', 'Grape White', 'Grape White 2', 'Grape White 3', 'Grape White 4', 'Grapefruit Pink', 'Grapefruit White', 'Guava', 'Hazelnut', 'Huckleberry', 'Kaki', 'Kiwi', 'Kohlrabi', 'Kumquats', 'Lemon', 'Lemon Meyer', 'Limes', 'Lychee', 'Mandarine', 'Mango', 'Mango Red', 'Mangostan', 'Maracuja', 'Melon Piel de Sapo', 'Mulberry', 'Nectarine', 'Nectarine Flat', 'Nut Forest', 'Nut Pecan', 'Onion Red', 'Onion Red Peeled', 'Onion White', 'Orange', 'Papaya', 'Passion Fruit', 'Peach', 'Peach 2', 'Peach Flat', 'Pear', 'Pear Abate', 'Pear Kaiser', 'Pear Monster', 'Pear Red', 'Pear Williams', 'Pepino', 'Pepper Green', 'Pepper Red', 'Pepper Yellow', 'Physalis', 'Physalis with Husk', 'Pineapple', 'Pineapple Mini', 'Pitahaya Red', 'Plum', 'Plum 2', 'Plum 3', 'Pomegranate', 'Pomelo Sweetie', 'Potato Red Washed', 'Potato White', 'Quince', 'Rambutan', 'Raspberry', 'Redcurrant', 'Salak', 'Strawberry', 'Strawberry Wedge', 'Tamarillo', 'Tangelo', 'Tomato 1', 'Tomato 2', 'Tomato 3', 'Tomato 4', 'Tomato Cherry Red', 'Tomato Maroon', 'Tomato Yellow', 'Walnut']\n",
      "Class count 114\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dir = os.getcwd() + '/images'\n",
    "\n",
    "image_datasets_scratch = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                              for x in ['Training', 'Test']}\n",
    "\n",
    "\n",
    "print(\"Training: %d\" % (len(image_datasets_scratch['Training'])))\n",
    "print(\"Testing %d\" % (len(image_datasets_scratch['Test'])))\n",
    "\n",
    "class_names = image_datasets_scratch['Training'].classes\n",
    "class_count = len(class_names)\n",
    "print(class_names)\n",
    "print(\"Class count %d\" % (class_count))\n",
    "                      \n",
    "batch_size = 20\n",
    "num_workers = 0\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(image_datasets_scratch['Training'])\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "loaders_scratch = {}\n",
    "loaders_scratch['Training'] = torch.utils.data.DataLoader(image_datasets_scratch['Training'], batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "loaders_scratch['Validation'] = torch.utils.data.DataLoader(image_datasets_scratch['Training'], batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "loaders_scratch['Test'] = torch.utils.data.DataLoader(image_datasets_scratch['Test'], batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN class that will classify our fruits. We will use 5 convolutional layers with max pooling layer after each. We will then put that through 2 fully connected layers.  We will use a Kernel of 3x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.2)\n",
      "  (conv_bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=2304, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=114, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    ### TODO: choose an architecture, and complete the class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## Define layers of a CNN\n",
    "        # convolutional layers\n",
    "         # convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding = 1 )       \n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding = 1 )\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding = 1 )\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, padding = 1 )\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3, padding = 1 )\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self.conv_bn2 = nn.BatchNorm2d(16)\n",
    "        self.conv_bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv_bn4 = nn.BatchNorm2d(64)\n",
    "        self.conv_bn5 = nn.BatchNorm2d(128)\n",
    "        self.conv_bn6 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 * 3 * 3, 500)\n",
    "        self.fc2 = nn.Linear(500, class_count)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.conv_bn2(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.conv_bn3(x)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.conv_bn4(x)\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.conv_bn5(x)\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = self.conv_bn6(x)\n",
    "        \n",
    "        # flatten image input\n",
    "        x = x.view(-1, 256 * 3 * 3)        \n",
    "       \n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net()\n",
    "print(model_scratch)\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model_scratch.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
    "\n",
    "Use the next code cell to specify a [loss function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/stable/optim.html).  Save the chosen loss function as `criterion_scratch`, and the optimizer as `optimizer_scratch` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### TODO: select loss function. This will penalize incorrect probabilities\n",
    "criterion_scratch = nn.CrossEntropyLoss()\n",
    "optimizer_scratch = optim.Adam(model_scratch.parameters(), lr=0.0008)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train and Validate the Model\n",
    "\n",
    "Train and validate your model in the code cell below.  [Save the final model parameters](http://pytorch.org/docs/master/notes/serialization.html) at the given file input path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(loaders_scratch['Training'])\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    print('Training started')\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        print('Started epoch')\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['Training']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "      \n",
    "            optimizer.zero_grad()\n",
    "            # Get output\n",
    "            output = model(data)               \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            train_loss = train_loss + (1 / (batch_idx + 1)) * (loss.data - train_loss)\n",
    "            \n",
    "            # Track the accuracy\n",
    "            total = target.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            acc_list.append(correct / total)\n",
    "\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                      .format(epoch + 1, n_epochs, batch_idx + 1, total_step, loss.item(),\n",
    "                              (correct / total) * 100))\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['Validation']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss = valid_loss + (1 / (batch_idx + 1)) * (loss.data - valid_loss)\n",
    "        \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Saving model: {} \\tNew Valid Loss: {:.6f} \\tPrevious Valid Loss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                valid_loss,\n",
    "                valid_loss_min\n",
    "                ))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss     \n",
    "    # return trained model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train and Validate the Model\n",
    "\n",
    "We will be training the model over 25 epochs and saving our models state to a local path.  We can see that the model validation loss is optimal around epoch 17 where the last model parameters get saved.  We will now use this model to run through our test function.  Our metric to optimize is log loss and we can see the best results are around 0.06 of loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.221388 \tValidation Loss: 0.495568\n",
      "Saving model: 1 \tNew Valid Loss: 0.495568 \tPrevious Valid Loss: inf\n",
      "Epoch: 2 \tTraining Loss: 0.540813 \tValidation Loss: 0.352751\n",
      "Saving model: 2 \tNew Valid Loss: 0.352751 \tPrevious Valid Loss: 0.495568\n",
      "Epoch: 3 \tTraining Loss: 0.410848 \tValidation Loss: 0.408099\n",
      "Epoch: 4 \tTraining Loss: 0.352851 \tValidation Loss: 0.195776\n",
      "Saving model: 4 \tNew Valid Loss: 0.195776 \tPrevious Valid Loss: 0.352751\n",
      "Epoch: 5 \tTraining Loss: 0.305918 \tValidation Loss: 0.254082\n",
      "Epoch: 6 \tTraining Loss: 0.268454 \tValidation Loss: 0.192788\n",
      "Saving model: 6 \tNew Valid Loss: 0.192788 \tPrevious Valid Loss: 0.195776\n",
      "Epoch: 7 \tTraining Loss: 0.252264 \tValidation Loss: 0.111330\n",
      "Saving model: 7 \tNew Valid Loss: 0.111330 \tPrevious Valid Loss: 0.192788\n",
      "Epoch: 8 \tTraining Loss: 0.230439 \tValidation Loss: 0.154823\n",
      "Epoch: 9 \tTraining Loss: 0.216887 \tValidation Loss: 0.105943\n",
      "Saving model: 9 \tNew Valid Loss: 0.105943 \tPrevious Valid Loss: 0.111330\n",
      "Epoch: 10 \tTraining Loss: 0.193004 \tValidation Loss: 0.122968\n",
      "Epoch: 11 \tTraining Loss: 0.196265 \tValidation Loss: 0.150928\n",
      "Epoch: 12 \tTraining Loss: 0.207260 \tValidation Loss: 0.085096\n",
      "Saving model: 12 \tNew Valid Loss: 0.085096 \tPrevious Valid Loss: 0.105943\n",
      "Epoch: 13 \tTraining Loss: 0.163085 \tValidation Loss: 0.099825\n",
      "Epoch: 14 \tTraining Loss: 0.169905 \tValidation Loss: 0.100911\n",
      "Epoch: 15 \tTraining Loss: 0.158628 \tValidation Loss: 0.126428\n",
      "Epoch: 16 \tTraining Loss: 0.159142 \tValidation Loss: 0.116617\n",
      "Epoch: 17 \tTraining Loss: 0.148059 \tValidation Loss: 0.066773\n",
      "Saving model: 17 \tNew Valid Loss: 0.066773 \tPrevious Valid Loss: 0.085096\n",
      "Epoch: 18 \tTraining Loss: 0.143657 \tValidation Loss: 0.071829\n",
      "Epoch: 19 \tTraining Loss: 0.135845 \tValidation Loss: 0.070809\n",
      "Epoch: 20 \tTraining Loss: 0.132916 \tValidation Loss: 0.095411\n",
      "Epoch: 21 \tTraining Loss: 0.134363 \tValidation Loss: 0.087179\n",
      "Epoch: 22 \tTraining Loss: 0.126599 \tValidation Loss: 0.072662\n",
      "Epoch: 23 \tTraining Loss: 0.126184 \tValidation Loss: 0.072023\n",
      "Epoch: 24 \tTraining Loss: 0.122310 \tValidation Loss: 0.068874\n",
      "Epoch: 25 \tTraining Loss: 0.121214 \tValidation Loss: 0.090195\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# train the model\n",
    "model_scratch = train(25, loaders_scratch, model_scratch, optimizer_scratch, criterion_scratch, use_cuda, 'model_fruit_scratch.pt')\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model_scratch.load_state_dict(torch.load('model_fruit_scratch.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test model\n",
    "\n",
    "We will be use the data set in the images/Test folder to test our newly created model.  We see that both accuracy and log loss looks to be in a good state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['Test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        #pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        #correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        #total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.029805\n",
      "\n",
      "\n",
      "Test Accuracy: 99% (19375/19548)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# call test function\n",
    "model_scratch.load_state_dict(torch.load('model_fruit_scratch.pt'))\n",
    "test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Create a CNN to Classify Fruit types (using Transfer Learning)\n",
    "\n",
    "We attempted to use transfer learning as well, even though our results from model from scratch where good.  I did a few passes but every time the model took a very long time to train, which was one of the reasons that my GPU credits with Udacity workspace ran out.  I also tried the training process in sagemaker and that also took a very long time.  The model implemenation is outlined below as it had been outlined below.  I used resnet and VGG16 and they were both not succesful training processes.  I only replaced the last layer in the model in both instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Use transfer learning to create a CNN to classify fruit type.  Use the code cell below, and save your initialized model as the variable `model_transfer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/generic/THCTensorCopy.c:20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-cb9cc20f8bf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel_transfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_transfer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/generic/THCTensorCopy.c:20"
     ]
    }
   ],
   "source": [
    "model_transfer = models.resnet50(pretrained=True)\n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "fc_inputs = model_transfer.fc.in_features\n",
    " \n",
    "model_transfer.fc = nn.Sequential(\n",
    "    nn.Linear(fc_inputs, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, 10), \n",
    "    nn.LogSoftmax(dim=1) # For using NLLLoss()\n",
    ")\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    model_transfer.cuda()\n",
    "print(model_transfer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Specify Loss Function and Optimizer\n",
    "\n",
    "Use the next code cell to specify a [loss function](http://pytorch.org/docs/master/nn.html#loss-functions) and [optimizer](http://pytorch.org/docs/master/optim.html).  Save the chosen loss function as `criterion_transfer`, and the optimizer as `optimizer_transfer` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_transfer = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_transfer = optim.Adam(model_scratch.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train and Validate the Model\n",
    "\n",
    "We will have to convert the images to the same input size that pretrained models expect, 224x224 pixels.  We will save these new model parameters in a new model.  This process was able to start but made my GPU credits run out so i had to stop it.  We will use our custom CNN to test below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "Started epoch\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/generated/../THCReduceAll.cuh:339",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b2c4221ddbc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# load the model that got the best validation accuracy (uncomment the line below)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mmodel_transfer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders_transfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_transfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_transfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_transfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_transfer.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mmodel_transfer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_transfer.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-33ec2a7a06f3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0macc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/generated/../THCReduceAll.cuh:339"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "n_epochs = 20\n",
    "\n",
    "normalize_transfer = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "resize_transform = transforms.Resize(256)\n",
    "centercrop_transform = transforms.CenterCrop(224)\n",
    "\n",
    "data_transforms_transfer = {\n",
    "    'Training': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        normalize_transfer\n",
    "    ]),\n",
    "    'Validation': transforms.Compose([\n",
    "        resize_transform,\n",
    "        centercrop_transform,\n",
    "        transforms.ToTensor(),\n",
    "        normalize_transfer\n",
    "    ]), \n",
    "    'Test': transforms.Compose([\n",
    "        resize_transform,\n",
    "        centercrop_transform,\n",
    "        transforms.ToTensor(),\n",
    "        normalize_transfer\n",
    "    ]),    \n",
    "}\n",
    "image_datasets_transfer = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms_transfer[x])\n",
    "                              for x in ['Training', 'Test']}\n",
    "\n",
    "# prepare data loaders\n",
    "loaders_transfer = {}\n",
    "loaders_transfer['Training'] = torch.utils.data.DataLoader(image_datasets_transfer['Training'], batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "loaders_transfer['Validation'] = torch.utils.data.DataLoader(image_datasets_transfer['Training'], batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "loaders_transfer['Test'] = torch.utils.data.DataLoader(image_datasets_transfer['Test'], batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# load the model that got the best validation accuracy (uncomment the line below)\n",
    "model_transfer = train(n_epochs, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, use_cuda, 'model_transfer.pt')\n",
    "\n",
    "model_transfer = torch.load('model_transfer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a previously trained model settings\n",
    "model_transfer.load_state_dict(torch.load('model_transfer.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of fruit images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.328588\n",
      "\n",
      "\n",
      "Test Accuracy: 89% (752/836)\n"
     ]
    }
   ],
   "source": [
    "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Fruit type with the Model\n",
    "\n",
    "Write a function that takes an image path as input and model parameter file and returns the fruit that is predicted by your model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_fruit_transfer(img_path, model_path):\n",
    "    # load the image and return the predicted breed\n",
    "\n",
    "    resize_transform = transforms.Resize(100)\n",
    "    centercrop_transform = transforms.CenterCrop(100)\n",
    "    normalize_transform = transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "    \n",
    "    transformation = transforms.Compose([\n",
    "        resize_transform,   \n",
    "        centercrop_transform,\n",
    "        transforms.ToTensor(),\n",
    "        normalize_transform\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(img_path)\n",
    "     # Preprocess the image\n",
    "    image_transform = transformation(image).float()\n",
    "\n",
    "    # Add extra dimension to treat as batch\n",
    "    image_transform = image_transform.unsqueeze_(0)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if use_cuda:\n",
    "        image = Variable(image_transform.cuda()) \n",
    "    else:\n",
    "        image = Variable(image_transform.to(device))\n",
    "    #Send to GPU if available\n",
    "    image_transform = image_transform.to(device)  \n",
    "    # Turn the input into a Variable\n",
    "    image_model_input = Variable(image_transform)\n",
    "    \n",
    "    model = Net()\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    \n",
    "    model_params = torch.load(model_path, map_location=torch.device(device))\n",
    "    model.load_state_dict(model_params)\n",
    "\n",
    "    #Prepare model\n",
    "    model.eval()\n",
    "    output = model(image_model_input)\n",
    "    \n",
    "    #Get values and index of max value\n",
    "    values, predict_index = torch.max(output, 1)\n",
    "    \n",
    "    return predict_index, classes[predict_index]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Net:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([16, 3, 5, 5]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-6fbf3e9b91bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Predicted fruit type: {fruit_class}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mrun_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"banana.jpeg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-6fbf3e9b91bd>\u001b[0m in \u001b[0;36mrun_app\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m## handle cases for a human face, dog, and neither\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpred_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfruit_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_fruit_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_fruit_scratch.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# extract breed from image path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-8715b1541b90>\u001b[0m in \u001b[0;36mpredict_fruit_transfer\u001b[0;34m(img_path, model_path)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mmodel_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m#Prepare model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 777\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    778\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Net:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([16, 3, 5, 5]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3])."
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.image as mpimg\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "def run_app(img_path):\n",
    "    ## handle cases for a human face, dog, and neither\n",
    "    pred_index, fruit_class = predict_fruit_transfer(img_path, 'model_fruit_scratch.pt')\n",
    "    \n",
    "    # extract breed from image path\n",
    "    actual_fruit = img_path.split(\"/\")[-1].split('_')[:-1]\n",
    "    actual_fruit = \" \".join(actual_fruit)\n",
    "    \n",
    "    # display test image\n",
    "    fig = plt.figure(figsize=(16,4))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    img = mpimg.imread(img_path)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # display sample of matching breed images\n",
    "    base_path = os.getcwd() + '/images/Validation'\n",
    "    fruit_directory =  '/'.join([base_path, str(fruit_class)])\n",
    "    fruit_file = random.choice(os.listdir(fruit_directory))\n",
    "    fruit_path = '/'.join([fruit_directory, fruit_file])\n",
    "    \n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    img = mpimg.imread(fruit_path)\n",
    "    ax.imshow(img.squeeze())\n",
    "    \n",
    "    fruit_class = fruit_class.split(\".\")[-1].replace(\"_\", \" \")\n",
    "    plt.title(breed_class)\n",
    "    plt.show()   \n",
    "    \n",
    "    print(f\"Input fruit type: {actual_fruit}\\n\")\n",
    "    print(f\"Predicted fruit type: {fruit_class}\\n\")\n",
    "\n",
    "run_app(\"banana.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (SageMaker) Model Training in SageMaker - Bonus due to GPU training running out\n",
    "\n",
    "Initially, the tests that were run where done in the Udacity Workspace. All my code above had been ran and tweeked successfully but had a few more things to test. My GPU balance ran out but still had some tests to run, so i created a \"source_pytorch\" folder, where I have my training script.  I was successfully able to run my training script.  Assumption is that the image files from Kaggle have been downloaded and placed in the workspace.  You'll have to uncomment a line below where the files get uploaded to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-979781493470/fruit_images\n",
      "s3://sagemaker-us-east-2-979781493470/fruit_images\n",
      "2019-08-07 04:37:42 Starting - Starting the training job...\n",
      "2019-08-07 04:37:45 Starting - Launching requested ML instances......\n",
      "2019-08-07 04:38:46 Starting - Preparing the instances for training...\n",
      "2019-08-07 04:39:33 Downloading - Downloading input data..........................................\n",
      "2019-08-07 04:46:42 Training - Downloading the training image..\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-08-07 04:46:57,252 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-08-07 04:46:57,254 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-08-07 04:46:57,267 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-08-07 04:46:57,268 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-08-07 04:46:57,592 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-08-07 04:46:57,592 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-08-07 04:46:57,593 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-08-07 04:46:57,593 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lv6h_g3c/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.2.1 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-08-07 04:46:59,731 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-08-07 04:46:59,743 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"lr\": 0.0008,\n",
      "        \"class_count\": 114,\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-08-07-04-37-41-909\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-979781493470/sagemaker-pytorch-2019-08-07-04-37-41-909/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"class_count\":114,\"epochs\":10,\"lr\":0.0008}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-east-2-979781493470/sagemaker-pytorch-2019-08-07-04-37-41-909/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"class_count\":114,\"epochs\":10,\"lr\":0.0008},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-08-07-04-37-41-909\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-979781493470/sagemaker-pytorch-2019-08-07-04-37-41-909/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--class_count\",\"114\",\"--epochs\",\"10\",\"--lr\",\"0.0008\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31mSM_HP_LR=0.0008\u001b[0m\n",
      "\u001b[31mSM_HP_CLASS_COUNT=114\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --class_count 114 --epochs 10 --lr 0.0008\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mUsing device cpu.\u001b[0m\n",
      "\u001b[31mGet train data loader.\u001b[0m\n",
      "\u001b[31m/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m['Apple Braeburn', 'Apple Crimson Snow', 'Apple Golden 1', 'Apple Golden 2', 'Apple Golden 3', 'Apple Granny Smith', 'Apple Pink Lady', 'Apple Red 1', 'Apple Red 2', 'Apple Red 3', 'Apple Red Delicious', 'Apple Red Yellow 1', 'Apple Red Yellow 2', 'Apricot', 'Avocado', 'Avocado ripe', 'Banana', 'Banana Lady Finger', 'Banana Red', 'Blueberry', 'Cactus fruit', 'Cantaloupe 1', 'Cantaloupe 2', 'Carambula', 'Cherry 1', 'Cherry 2', 'Cherry Rainier', 'Cherry Wax Black', 'Cherry Wax Red', 'Cherry Wax Yellow', 'Chestnut', 'Clementine', 'Cocos', 'Dates', 'Ginger Root', 'Granadilla', 'Grape Blue', 'Grape Pink', 'Grape White', 'Grape White 2', 'Grape White 3', 'Grape White 4', 'Grapefruit Pink', 'Grapefruit White', 'Guava', 'Hazelnut', 'Huckleberry', 'Kaki', 'Kiwi', 'Kohlrabi', 'Kumquats', 'Lemon', 'Lemon Meyer', 'Limes', 'Lychee', 'Mandarine', 'Mango', 'Mango Red', 'Mangostan', 'Maracuja', 'Melon Piel de Sapo', 'Mulberry', 'Nectarine', 'Nectarine Flat', 'Nut Forest', 'Nut Pecan', 'Onion Red', 'Onion Red Peeled', 'Onion White', 'Orange', 'Papaya', 'Passion Fruit', 'Peach', 'Peach 2', 'Peach Flat', 'Pear', 'Pear Abate', 'Pear Kaiser', 'Pear Monster', 'Pear Red', 'Pear Williams', 'Pepino', 'Pepper Green', 'Pepper Red', 'Pepper Yellow', 'Physalis', 'Physalis with Husk', 'Pineapple', 'Pineapple Mini', 'Pitahaya Red', 'Plum', 'Plum 2', 'Plum 3', 'Pomegranate', 'Pomelo Sweetie', 'Potato Red Washed', 'Potato White', 'Quince', 'Rambutan', 'Raspberry', 'Redcurrant', 'Salak', 'Strawberry', 'Strawberry Wedge', 'Tamarillo', 'Tangelo', 'Tomato 1', 'Tomato 2', 'Tomato 3', 'Tomato 4', 'Tomato Cherry Red', 'Tomato Maroon', 'Tomato Yellow', 'Walnut']\u001b[0m\n",
      "\u001b[31mClass count 114\u001b[0m\n",
      "\u001b[31mStarted epoch\u001b[0m\n",
      "\n",
      "2019-08-07 04:46:54 Training - Training image download completed. Training in progress.\u001b[31mEpoch [2/10], Step [100/2292], Loss: 2.4806, Accuracy: 20.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [200/2292], Loss: 1.9817, Accuracy: 30.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [300/2292], Loss: 1.9208, Accuracy: 35.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [400/2292], Loss: 2.4374, Accuracy: 45.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [500/2292], Loss: 1.1893, Accuracy: 55.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [600/2292], Loss: 1.0929, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [700/2292], Loss: 1.4460, Accuracy: 60.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [800/2292], Loss: 1.5014, Accuracy: 70.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [900/2292], Loss: 0.7829, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1000/2292], Loss: 1.6549, Accuracy: 50.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1100/2292], Loss: 1.0419, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1200/2292], Loss: 0.7288, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1300/2292], Loss: 1.4539, Accuracy: 60.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1400/2292], Loss: 0.6624, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1500/2292], Loss: 0.8542, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1600/2292], Loss: 0.6082, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1700/2292], Loss: 0.7555, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1800/2292], Loss: 0.3725, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1900/2292], Loss: 1.0688, Accuracy: 70.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [2000/2292], Loss: 0.4475, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [2100/2292], Loss: 0.7112, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [2200/2292], Loss: 0.3895, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch: 1 #011Training Loss: 1.171827 #011Validation Loss: 1.337489\u001b[0m\n",
      "\u001b[31mSaving model: 1 #011New Valid Loss: 1.337489 #011Previous Valid Loss: inf\u001b[0m\n",
      "\u001b[31mStarted epoch\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [100/2292], Loss: 0.5487, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [200/2292], Loss: 0.5683, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [300/2292], Loss: 1.0715, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [400/2292], Loss: 0.5602, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [500/2292], Loss: 0.2754, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [600/2292], Loss: 0.6069, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [700/2292], Loss: 0.1294, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [800/2292], Loss: 1.1408, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [900/2292], Loss: 0.3694, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1000/2292], Loss: 0.9811, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1100/2292], Loss: 0.5377, Accuracy: 70.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1200/2292], Loss: 0.2551, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1300/2292], Loss: 0.4347, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1400/2292], Loss: 0.9205, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1500/2292], Loss: 0.5034, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1600/2292], Loss: 0.4095, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1700/2292], Loss: 0.2015, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1800/2292], Loss: 0.5048, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1900/2292], Loss: 0.6686, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [2000/2292], Loss: 0.1800, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [2100/2292], Loss: 0.8945, Accuracy: 70.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [2200/2292], Loss: 0.4991, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch: 2 #011Training Loss: 0.547197 #011Validation Loss: 0.304914\u001b[0m\n",
      "\u001b[31mSaving model: 2 #011New Valid Loss: 0.304914 #011Previous Valid Loss: 1.337489\u001b[0m\n",
      "\u001b[31mStarted epoch\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [100/2292], Loss: 0.4953, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [200/2292], Loss: 0.6292, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [300/2292], Loss: 0.2100, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [400/2292], Loss: 1.4243, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [500/2292], Loss: 0.3089, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [600/2292], Loss: 0.1303, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [700/2292], Loss: 0.2462, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [800/2292], Loss: 0.5076, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [900/2292], Loss: 0.0580, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [1000/2292], Loss: 0.2428, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [1100/2292], Loss: 0.8905, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [1200/2292], Loss: 0.8525, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [1300/2292], Loss: 0.6018, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [1400/2292], Loss: 0.3267, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [1500/2292], Loss: 0.4724, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [1600/2292], Loss: 0.2186, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [1700/2292], Loss: 0.5222, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [1800/2292], Loss: 0.6863, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [1900/2292], Loss: 0.7565, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [2000/2292], Loss: 0.3111, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [2100/2292], Loss: 0.5158, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [4/10], Step [2200/2292], Loss: 0.1442, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch: 3 #011Training Loss: 0.422104 #011Validation Loss: 0.325390\u001b[0m\n",
      "\u001b[31mStarted epoch\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [100/2292], Loss: 0.5291, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [200/2292], Loss: 0.3246, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [300/2292], Loss: 0.2736, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [400/2292], Loss: 0.5975, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [500/2292], Loss: 0.2264, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [600/2292], Loss: 0.5176, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [700/2292], Loss: 0.5117, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [800/2292], Loss: 0.3774, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [900/2292], Loss: 0.0688, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [1000/2292], Loss: 0.1835, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [1100/2292], Loss: 0.2273, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [1200/2292], Loss: 0.4450, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [1300/2292], Loss: 0.3653, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [1400/2292], Loss: 0.4360, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [1500/2292], Loss: 0.3956, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [1600/2292], Loss: 0.6256, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [1700/2292], Loss: 0.4357, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [1800/2292], Loss: 0.0328, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [1900/2292], Loss: 0.0241, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [2000/2292], Loss: 0.5209, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [2100/2292], Loss: 0.1894, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [5/10], Step [2200/2292], Loss: 0.1755, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch: 4 #011Training Loss: 0.356436 #011Validation Loss: 0.226281\u001b[0m\n",
      "\u001b[31mSaving model: 4 #011New Valid Loss: 0.226281 #011Previous Valid Loss: 0.304914\u001b[0m\n",
      "\u001b[31mStarted epoch\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [100/2292], Loss: 0.1525, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [200/2292], Loss: 0.8235, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [300/2292], Loss: 0.2896, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [400/2292], Loss: 0.9107, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [500/2292], Loss: 0.1406, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [600/2292], Loss: 0.2359, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [700/2292], Loss: 0.3055, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [800/2292], Loss: 0.0903, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [900/2292], Loss: 0.2171, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [1000/2292], Loss: 0.3147, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [1100/2292], Loss: 0.0419, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [1200/2292], Loss: 0.3463, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [1300/2292], Loss: 0.0516, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [1400/2292], Loss: 0.1472, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [1500/2292], Loss: 0.4767, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [1600/2292], Loss: 0.3304, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [1700/2292], Loss: 0.2754, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [1800/2292], Loss: 0.4445, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [1900/2292], Loss: 0.1509, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [2000/2292], Loss: 0.1589, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [2100/2292], Loss: 0.4712, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [6/10], Step [2200/2292], Loss: 0.5550, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch: 5 #011Training Loss: 0.314411 #011Validation Loss: 0.178017\u001b[0m\n",
      "\u001b[31mSaving model: 5 #011New Valid Loss: 0.178017 #011Previous Valid Loss: 0.226281\u001b[0m\n",
      "\u001b[31mStarted epoch\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [100/2292], Loss: 0.0941, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [200/2292], Loss: 0.1610, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [300/2292], Loss: 0.0713, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [400/2292], Loss: 0.2898, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [500/2292], Loss: 0.6059, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [600/2292], Loss: 0.5169, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [700/2292], Loss: 0.1277, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [800/2292], Loss: 0.4169, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [900/2292], Loss: 0.2922, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [1000/2292], Loss: 0.4549, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [1100/2292], Loss: 0.0718, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [1200/2292], Loss: 0.5052, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [1300/2292], Loss: 0.0430, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [1400/2292], Loss: 0.1532, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [1500/2292], Loss: 0.1622, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [1600/2292], Loss: 0.0392, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [1700/2292], Loss: 0.0807, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [1800/2292], Loss: 0.1081, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [1900/2292], Loss: 0.3453, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [2000/2292], Loss: 0.1844, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [2100/2292], Loss: 0.2421, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [7/10], Step [2200/2292], Loss: 0.2519, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch: 6 #011Training Loss: 0.287392 #011Validation Loss: 0.338559\u001b[0m\n",
      "\u001b[31mStarted epoch\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [100/2292], Loss: 0.1973, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [200/2292], Loss: 0.4146, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [300/2292], Loss: 0.3070, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [400/2292], Loss: 0.0546, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [500/2292], Loss: 0.2021, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [600/2292], Loss: 0.5945, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [700/2292], Loss: 0.4868, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [800/2292], Loss: 0.1253, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [900/2292], Loss: 0.7261, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [1000/2292], Loss: 0.2767, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [1100/2292], Loss: 0.1313, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [1200/2292], Loss: 0.3788, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [1300/2292], Loss: 0.2344, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [1400/2292], Loss: 0.1430, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [1500/2292], Loss: 0.1848, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [1600/2292], Loss: 0.2026, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [1700/2292], Loss: 1.0804, Accuracy: 60.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [1800/2292], Loss: 0.1699, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [1900/2292], Loss: 0.0844, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [2000/2292], Loss: 0.0089, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [2100/2292], Loss: 0.0801, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [8/10], Step [2200/2292], Loss: 0.8377, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch: 7 #011Training Loss: 0.263432 #011Validation Loss: 0.329837\u001b[0m\n",
      "\u001b[31mStarted epoch\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [100/2292], Loss: 0.3387, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [200/2292], Loss: 0.1503, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [300/2292], Loss: 0.3069, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [400/2292], Loss: 0.4545, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [500/2292], Loss: 0.5074, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [600/2292], Loss: 0.0620, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [700/2292], Loss: 0.1659, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [800/2292], Loss: 0.0632, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [900/2292], Loss: 0.1687, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [1000/2292], Loss: 0.1731, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [1100/2292], Loss: 0.0658, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [1200/2292], Loss: 0.1790, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [1300/2292], Loss: 0.0879, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [1400/2292], Loss: 0.6086, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [1500/2292], Loss: 0.2403, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [1600/2292], Loss: 0.2033, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [1700/2292], Loss: 0.5031, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [1800/2292], Loss: 0.0720, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [1900/2292], Loss: 0.1966, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [2000/2292], Loss: 0.0629, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [2100/2292], Loss: 0.2878, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [9/10], Step [2200/2292], Loss: 0.2499, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch: 8 #011Training Loss: 0.240918 #011Validation Loss: 0.206497\u001b[0m\n",
      "\u001b[31mStarted epoch\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [100/2292], Loss: 0.2210, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [200/2292], Loss: 0.5170, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [300/2292], Loss: 0.1258, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [400/2292], Loss: 0.6097, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [500/2292], Loss: 0.3180, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [600/2292], Loss: 0.3708, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [700/2292], Loss: 0.2467, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [800/2292], Loss: 0.0541, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [900/2292], Loss: 0.1341, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [1000/2292], Loss: 0.1932, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [1100/2292], Loss: 0.2645, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [1200/2292], Loss: 0.3168, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [1300/2292], Loss: 0.1401, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [1400/2292], Loss: 0.2009, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [1500/2292], Loss: 0.3015, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [1600/2292], Loss: 0.0463, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [1700/2292], Loss: 0.0945, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [1800/2292], Loss: 0.0046, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [1900/2292], Loss: 0.0193, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [2000/2292], Loss: 0.1311, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [2100/2292], Loss: 0.0070, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [10/10], Step [2200/2292], Loss: 0.4150, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch: 9 #011Training Loss: 0.224715 #011Validation Loss: 0.140016\u001b[0m\n",
      "\u001b[31mSaving model: 9 #011New Valid Loss: 0.140016 #011Previous Valid Loss: 0.178017\u001b[0m\n",
      "\u001b[31mStarted epoch\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [100/2292], Loss: 0.4950, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [200/2292], Loss: 0.3842, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [300/2292], Loss: 0.2405, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [400/2292], Loss: 0.3126, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [500/2292], Loss: 0.2455, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [600/2292], Loss: 0.0141, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [700/2292], Loss: 0.2887, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [800/2292], Loss: 0.2394, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [900/2292], Loss: 0.0482, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [1000/2292], Loss: 0.1291, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [1100/2292], Loss: 0.0190, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [1200/2292], Loss: 0.1186, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [1300/2292], Loss: 0.2697, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [1400/2292], Loss: 0.4328, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [1500/2292], Loss: 0.1109, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [1600/2292], Loss: 0.1057, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [1700/2292], Loss: 0.3339, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [1800/2292], Loss: 0.0545, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [1900/2292], Loss: 0.3321, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [2000/2292], Loss: 0.3766, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [2100/2292], Loss: 0.1087, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [11/10], Step [2200/2292], Loss: 0.0373, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch: 10 #011Training Loss: 0.209021 #011Validation Loss: 0.126094\u001b[0m\n",
      "\u001b[31mSaving model: 10 #011New Valid Loss: 0.126094 #011Previous Valid Loss: 0.140016\u001b[0m\n",
      "\u001b[31mFinished training. Testing now\u001b[0m\n",
      "\u001b[31mTest Loss: 0.049989\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mTest Accuracy: 98% (19261/19548)\u001b[0m\n",
      "\u001b[31mFinished testing.  Exiting\u001b[0m\n",
      "\u001b[31m2019-08-07 06:01:24,590 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-08-07 06:01:35 Uploading - Uploading generated training model\n",
      "2019-08-07 06:01:35 Completed - Training job completed\n",
      "Billable seconds: 4923\n"
     ]
    }
   ],
   "source": [
    "#Required to start.  Place kaggle downloaded images into \"images\" folder that can be accessed by the Jupyter notebook\n",
    "import boto3\n",
    "import sagemaker\n",
    "from PIL import ImageFile  \n",
    "from PIL import Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  \n",
    "\n",
    "# your import and estimator code, here\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# should be the name of directory you created to save your features data\n",
    "data_dir = 'images'\n",
    "\n",
    "# set prefix, a descriptive name for a directory  \n",
    "prefix = 'fruit_images'\n",
    "\n",
    "# upload all data to S3 - UNCOMMENT THIS LINE TO TEST\n",
    "#input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)\n",
    "input_data = \"s3://sagemaker-us-east-2-979781493470/fruit_images\"\n",
    "print(input_data)\n",
    "\n",
    "# specify an output path\n",
    "# prefix is specified above\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "print(output_path)\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir='source_pytorch', # this should be just \"source\" for your code\n",
    "                    role=role,\n",
    "                    framework_version='1.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.c4.xlarge',\n",
    "                    output_path=output_path,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    hyperparameters={\n",
    "                        'class_count': 114,\n",
    "                        'epochs': 10, # could change to higher\n",
    "                        'lr': 0.0008\n",
    "                    })\n",
    "estimator.fit({'train': input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-2-979781493470/fruit_images\n",
      "s3://sagemaker-us-east-2-979781493470/fruit_images\n",
      "2019-08-07 06:45:02 Starting - Starting the training job...\n",
      "2019-08-07 06:45:06 Starting - Launching requested ML instances......\n",
      "2019-08-07 06:46:08 Starting - Preparing the instances for training...\n",
      "2019-08-07 06:46:56 Downloading - Downloading input data.......................................\n",
      "2019-08-07 06:53:31 Training - Downloading the training image..\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-08-07 06:53:46,557 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-08-07 06:53:46,560 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-08-07 06:53:46,573 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-08-07 06:53:49,591 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-08-07 06:53:49,987 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-08-07 06:53:49,988 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-08-07 06:53:49,988 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-08-07 06:53:49,988 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\n",
      "2019-08-07 06:53:44 Training - Training image download completed. Training in progress.\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wht7bvwb/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.2.1 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-08-07 06:53:51,940 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-08-07 06:53:51,953 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"lr\": 0.0008,\n",
      "        \"class_count\": 114,\n",
      "        \"epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-08-07-06-45-01-878\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-979781493470/sagemaker-pytorch-2019-08-07-06-45-01-878/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"class_count\":114,\"epochs\":10,\"lr\":0.0008}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-east-2-979781493470/sagemaker-pytorch-2019-08-07-06-45-01-878/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"class_count\":114,\"epochs\":10,\"lr\":0.0008},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-08-07-06-45-01-878\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-979781493470/sagemaker-pytorch-2019-08-07-06-45-01-878/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--class_count\",\"114\",\"--epochs\",\"10\",\"--lr\",\"0.0008\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31mSM_HP_LR=0.0008\u001b[0m\n",
      "\u001b[31mSM_HP_CLASS_COUNT=114\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --class_count 114 --epochs 10 --lr 0.0008\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mUsing device cpu.\u001b[0m\n",
      "\u001b[31mGet train data loader.\u001b[0m\n",
      "\u001b[31m/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m['Apple Braeburn', 'Apple Crimson Snow', 'Apple Golden 1', 'Apple Golden 2', 'Apple Golden 3', 'Apple Granny Smith', 'Apple Pink Lady', 'Apple Red 1', 'Apple Red 2', 'Apple Red 3', 'Apple Red Delicious', 'Apple Red Yellow 1', 'Apple Red Yellow 2', 'Apricot', 'Avocado', 'Avocado ripe', 'Banana', 'Banana Lady Finger', 'Banana Red', 'Blueberry', 'Cactus fruit', 'Cantaloupe 1', 'Cantaloupe 2', 'Carambula', 'Cherry 1', 'Cherry 2', 'Cherry Rainier', 'Cherry Wax Black', 'Cherry Wax Red', 'Cherry Wax Yellow', 'Chestnut', 'Clementine', 'Cocos', 'Dates', 'Ginger Root', 'Granadilla', 'Grape Blue', 'Grape Pink', 'Grape White', 'Grape White 2', 'Grape White 3', 'Grape White 4', 'Grapefruit Pink', 'Grapefruit White', 'Guava', 'Hazelnut', 'Huckleberry', 'Kaki', 'Kiwi', 'Kohlrabi', 'Kumquats', 'Lemon', 'Lemon Meyer', 'Limes', 'Lychee', 'Mandarine', 'Mango', 'Mango Red', 'Mangostan', 'Maracuja', 'Melon Piel de Sapo', 'Mulberry', 'Nectarine', 'Nectarine Flat', 'Nut Forest', 'Nut Pecan', 'Onion Red', 'Onion Red Peeled', 'Onion White', 'Orange', 'Papaya', 'Passion Fruit', 'Peach', 'Peach 2', 'Peach Flat', 'Pear', 'Pear Abate', 'Pear Kaiser', 'Pear Monster', 'Pear Red', 'Pear Williams', 'Pepino', 'Pepper Green', 'Pepper Red', 'Pepper Yellow', 'Physalis', 'Physalis with Husk', 'Pineapple', 'Pineapple Mini', 'Pitahaya Red', 'Plum', 'Plum 2', 'Plum 3', 'Pomegranate', 'Pomelo Sweetie', 'Potato Red Washed', 'Potato White', 'Quince', 'Rambutan', 'Raspberry', 'Redcurrant', 'Salak', 'Strawberry', 'Strawberry Wedge', 'Tamarillo', 'Tangelo', 'Tomato 1', 'Tomato 2', 'Tomato 3', 'Tomato 4', 'Tomato Cherry Red', 'Tomato Maroon', 'Tomato Yellow', 'Walnut']\u001b[0m\n",
      "\u001b[31mClass count 114\u001b[0m\n",
      "\u001b[31mStarted epoch\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [100/2292], Loss: 3.0131, Accuracy: 15.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [200/2292], Loss: 2.4750, Accuracy: 30.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [300/2292], Loss: 1.7946, Accuracy: 40.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [400/2292], Loss: 1.8295, Accuracy: 55.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [500/2292], Loss: 1.2665, Accuracy: 65.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [600/2292], Loss: 1.3203, Accuracy: 55.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [700/2292], Loss: 1.0399, Accuracy: 70.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [800/2292], Loss: 0.5368, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [900/2292], Loss: 0.9782, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1000/2292], Loss: 0.9227, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1100/2292], Loss: 1.4687, Accuracy: 65.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1200/2292], Loss: 0.7699, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1300/2292], Loss: 0.6646, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1400/2292], Loss: 0.6114, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1500/2292], Loss: 0.6587, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1600/2292], Loss: 0.6952, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1700/2292], Loss: 0.5303, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1800/2292], Loss: 0.7242, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [1900/2292], Loss: 0.6880, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [2000/2292], Loss: 0.4937, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [2100/2292], Loss: 0.5779, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [2/10], Step [2200/2292], Loss: 0.6007, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch: 1 #011Training Loss: 1.262694 #011Validation Loss: 0.575017\u001b[0m\n",
      "\u001b[31mSaving model: 1 #011New Valid Loss: 0.575017 #011Previous Valid Loss: inf\u001b[0m\n",
      "\u001b[31mStarted epoch\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [100/2292], Loss: 0.4495, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [200/2292], Loss: 0.3727, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [300/2292], Loss: 0.7544, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [400/2292], Loss: 0.6764, Accuracy: 75.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [500/2292], Loss: 0.6857, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [600/2292], Loss: 0.2573, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [700/2292], Loss: 0.2448, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [800/2292], Loss: 0.6484, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [900/2292], Loss: 0.0392, Accuracy: 100.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1000/2292], Loss: 0.1844, Accuracy: 95.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1100/2292], Loss: 0.4959, Accuracy: 85.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1200/2292], Loss: 0.3402, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1300/2292], Loss: 0.7060, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1400/2292], Loss: 0.7267, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1500/2292], Loss: 0.5906, Accuracy: 80.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1600/2292], Loss: 0.2368, Accuracy: 90.00%\u001b[0m\n",
      "\u001b[31mEpoch [3/10], Step [1700/2292], Loss: 0.1183, Accuracy: 100.00%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from PIL import ImageFile  \n",
    "from PIL import Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  \n",
    "\n",
    "# your import and estimator code, here\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# should be the name of directory you created to save your features data\n",
    "data_dir = 'images'\n",
    "\n",
    "# set prefix, a descriptive name for a directory  \n",
    "prefix = 'fruit_images'\n",
    "\n",
    "# upload all data to S3 - UNCOMMENT THIS LINE TO TEST\n",
    "#input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)\n",
    "input_data = \"s3://sagemaker-us-east-2-979781493470/fruit_images\"\n",
    "print(input_data)\n",
    "\n",
    "# specify an output path\n",
    "# prefix is specified above\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "print(output_path)\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir='source_pytorch', # this should be just \"source\" for your code\n",
    "                    role=role,\n",
    "                    framework_version='1.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.c4.xlarge',\n",
    "                    output_path=output_path,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    hyperparameters={\n",
    "                        'class_count': 114,\n",
    "                        'epochs': 10, # could change to higher\n",
    "                        'lr': 0.0008\n",
    "                    })\n",
    "estimator.fit({'train': input_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "# uncomment, if needed\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "model = PyTorchModel(model_data=estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='1.0',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='source_pytorch')\n",
    "\n",
    "# deploy your model to create a predictor\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
